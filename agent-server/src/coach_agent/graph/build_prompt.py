# coach_agent/graph/build_prompt.py
import yaml
from langchain_core.prompts import (
    ChatPromptTemplate, 
    HumanMessagePromptTemplate, 
    MessagesPlaceholder,
    SystemMessagePromptTemplate
)
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from coach_agent.state_types import State, CounselorTurn
from coach_agent.services import REPO
from coach_agent.prompts import (
    FIXED_NEW_USER_SCRIPT,
    TEMPLATE_GREETING_WEEKLY,
    TEMPLATE_GREETING_GENERAL,
    TEMPLATE_CONVERSATION
)

# --- í—¬í¼ í•¨ìˆ˜ ---

def _clean_message_content(msg: BaseMessage) -> BaseMessage:
    """ë©”ì‹œì§€ ì»¨í…ì¸ ê°€ ë¦¬ìŠ¤íŠ¸(ë©€í‹°ëª¨ë‹¬ ë“±)ì¼ ê²½ìš° í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë‹¨ìˆœí™”"""
    content = msg.content
    if isinstance(content, list):
        # í…ìŠ¤íŠ¸ íƒ€ì…ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ
        text_content = next(
            (item.get("text", "") for item in content if isinstance(item, dict) and item.get("type") == "text"), 
            ""
        )
        content = text_content
    
    # ë¬¸ìì—´ ë³´ì¥
    content_str = str(content) if not isinstance(content, str) else content

    if msg.type == "human":
        return HumanMessage(content=content_str)
    elif msg.type == "ai":
        return AIMessage(content=content_str)
    elif msg.type == "system":
        return SystemMessage(content=content_str)
    else:
        msg.content = content_str
        return msg

def _load_past_summaries(user_id: str, current_week: int) -> list:
    """ê³¼ê±° ì£¼ì°¨ ìš”ì•½ë³¸ì„ ê°€ì ¸ì™€ AIMessage í˜•íƒœë¡œ ë³€í™˜"""
    summaries = REPO.get_past_summaries(user_id, current_week)
    return [
        AIMessage(content=f"--- ì§€ë‚œ {s['week']}ì£¼ì°¨ ìš”ì•½ ---\n{s['summary']}")
        for s in summaries
    ]

def _format_steps(raw_steps: list) -> str:
    """ìŠ¤í… ë¦¬ìŠ¤íŠ¸ë¥¼ ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    return "\n".join([f"[{i}] {step}" for i, step in enumerate(raw_steps)])

# --- Main Function ---

def build_prompt(state: State) -> dict:
    spec = state.protocol
    session_type = state.session_type
    nickname = state.nickname

    # 1. ì²« í„´(ì¸ì‚¬) ì—¬ë¶€ í™•ì¸
    if state.last_user_message is None:
        # [Case A] ì²« í„´: ì¸ì‚¬ë§ ìƒì„± ëª¨ë“œ
        
        # 1-1. ì‹ ê·œ ì‚¬ìš©ì (ë‹‰ë„¤ì„ ì—†ìŒ)
        if nickname is None:
            manual_output = CounselorTurn(
                response_text=FIXED_NEW_USER_SCRIPT,
                session_goals_met=False,
                reasoning="ì‹ ê·œ ì‚¬ìš©ì ìµœì´ˆ ì§„ì…. ë‹‰ë„¤ì„ ìš”ì²­.",
                current_step_index=state.current_step_index or 0
            )
            # LLM í˜¸ì¶œ ì—†ì´ ë°”ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜ (ìˆ˜ë™ ì‘ë‹µ)
            return {
                "messages": [AIMessage(content=manual_output.response_text)],
                "session_goals_met": manual_output.session_goals_met,
                "reasoning": manual_output.reasoning, 
            }
            
        # 1-2. ê¸°ì¡´ ì‚¬ìš©ì: ì„¸ì…˜ íƒ€ì…ë³„ ì¸ì‚¬
        if session_type == "WEEKLY":
            template = TEMPLATE_GREETING_WEEKLY
            # prompt_seedê°€ ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
            seed_data = spec.get("prompt_seed", ["ì˜¤ëŠ˜ ì–´ë– ì…¨ë‚˜ìš”?"])
            seed_text = seed_data[0] if isinstance(seed_data, list) else seed_data

            variables = {
                "nickname": nickname,
                "days_since_last_seen": state.days_since_last_seen,
                "session_type": "WEEKLY",
                "week": spec.get("week", state.current_week),
                "title": spec.get("title", "ì£¼ê°„ ìƒë‹´"),
                "goals": "; ".join(spec.get("goals", [])),
                "prompt_seed": seed_text,
            }
            
        elif session_type == "GENERAL":
            template = TEMPLATE_GREETING_GENERAL
            variables = {"nickname": nickname}
        else:
            # Fallback
            template = "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n\n[ì¤‘ìš”] **ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.**"
            variables = {}

        prompt_messages = ChatPromptTemplate.from_template(template).invoke(variables).to_messages()

    else:
        # [Case B] ëŒ€í™” ì¤‘ (Conversation Loop)
        
        # 1. ê°œì… ë ˆë²¨ ì„¤ì •
        level = state.intervention_level or "L1"
        intervention_instruction = "Proceed with the standard CBT coaching flow."
        empathy_instruction = "Show empathy and acknowledge the Human's last message."

        if level in ["L4", "L5"]:
            intervention_instruction = """
            ğŸš¨ **EMERGENCY / HIGH RISK DETECTED** ğŸš¨
            The user is showing signs of severe depression.
            1. Suggest professional help gently.
            2. Gently steer them back to the topic.
            """
            empathy_instruction = "Show deep empathy and validate their pain heavily."
        elif level in ["L2", "L3"]:
            intervention_instruction = """
            âš ï¸ **AVOIDANCE DETECTED** âš ï¸
            Redirect immediately to the 'Script Steps'.
            """
            empathy_instruction = "Briefly acknowledge, but prioritize the session goal."

        # 2. ìŠ¤í… ë° ì¸ë±ìŠ¤ ê³„ì‚°
        raw_steps = spec.get("script_steps", [])
        current_idx = state.current_step_index or 0
        
        # ì¸ë±ìŠ¤ ë²”ìœ„ ë³´ì • (IndexOutOfBounds ë°©ì§€)
        if raw_steps:
            current_idx = min(current_idx, len(raw_steps) - 1)
            current_step_text = raw_steps[current_idx]
        else:
            current_step_text = "ììœ  ëŒ€í™”"

        # 3. í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ êµ¬ì„±
        variables = {
            "week": spec.get("week", state.current_week),
            "title": spec.get("title", "Daily Check-in"),
            "goals": "; ".join(spec.get("goals", [])),
            
            # ìŠ¤í… ë„¤ë¹„ê²Œì´ì…˜ ì •ë³´
            "steps": _format_steps(raw_steps),
            "total_steps": len(raw_steps),
            "current_step_index": current_idx,
            "current_step_text": current_step_text,
            "next_step_index": current_idx + 1,
            "prev_step_index": max(0, current_idx - 1),
            
            "level": level,
            "exit_goals": yaml.dump(spec.get("exit_criteria", {}), allow_unicode=True),
            "user_message": state.last_user_message,
            "intervention_instruction": intervention_instruction,
            "empathy_instruction": empathy_instruction,
            "nickname": nickname or "ì—¬í–‰ì",
        }
        
        # 4. íˆìŠ¤í† ë¦¬ ë¡œë“œ ë° ê²°í•©
        cleaned_chat_history = [_clean_message_content(msg) for msg in state.messages]
        past_summaries = _load_past_summaries(state.user_id, state.current_week)
        
        # 5. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt_template = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(TEMPLATE_CONVERSATION),
            MessagesPlaceholder(variable_name="history"),
            HumanMessagePromptTemplate.from_template("{user_message}"),
        ])
        
        # history ì£¼ì…
        invoke_vars = variables.copy()
        invoke_vars["history"] = past_summaries + cleaned_chat_history
        
        prompt_messages = prompt_template.invoke(invoke_vars).to_messages()
    
    return {
        "llm_prompt_messages": prompt_messages
    }