# coach_agent/graph/run_llm.py
from state_types import State
from services.llm import LLM_CHAIN # ë¯¸ë¦¬ ë¹Œë“œëœ ì²´ì¸ ì„í¬íŠ¸
from langchain_core.messages import AIMessage

def run_llm(state: State) -> dict:
    print(f"\n=== [DEBUG] RunLLM Node Started ===")
    
    # 1. LLM í˜¸ì¶œ (CounselorTurn ê°ì²´ ë°˜í™˜)
    structured_output = LLM_CHAIN.invoke(state.llm_prompt_messages)
    
    # --- [ë””ë²„ê¹… ì½”ë“œ ì¶”ê°€] ---
    print(f"ğŸ¤– LLM Response Generated:")
    print(f"   - Session Goals Met?: {structured_output.session_goals_met}")
    print(f"   - Reasoning (ê·¼ê±°): {structured_output.reasoning}")
    print(f"   - Assistant Says: {structured_output.response_text[:50]}...") # ë„ˆë¬´ ê¸°ë‹ˆê¹Œ ì•ë¶€ë¶„ë§Œ
    # -----------------------

    # 2. ìƒíƒœ ì—…ë°ì´íŠ¸
    # LLMì˜ ë‹µë³€ í…ìŠ¤íŠ¸ ì €ì¥
    state.llm_output = structured_output.response_text
    
    # â˜… í•µì‹¬: ì—¬ê¸°ì„œ LLMì˜ íŒë‹¨ì´ Stateì˜ exit ë³€ìˆ˜ë¡œ ë„˜ì–´ê°
    state.exit = structured_output.session_goals_met
    
    # (ì„ íƒì ) ê·¼ê±°ë¥¼ metricsì— ì €ì¥í•´ë‘ë©´ ë‚˜ì¤‘ì— ë¶„ì„ ê°€ëŠ¥
    if structured_output.reasoning:
        state.metrics["exit_reasoning"] = structured_output.reasoning

    # 3. ë©”ì‹œì§€ ì¶”ê°€ ë° ë°˜í™˜
    return {
        "messages": [AIMessage(content=structured_output.response_text)],
        # ì—…ë°ì´íŠ¸í•œ state.exitì˜ ê°’ ë°˜í™˜ -> ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬
        "exit": structured_output.session_goals_met,
        "llm_output": structured_output.response_text
    }