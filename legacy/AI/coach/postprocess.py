"""
postprocess.py  â€”  ìµœì¢… ì‘ë‹µì„ 'ì˜ˆì˜ê³  ì¼ê´€ë˜ê²Œ' ë‹¤ë“¬ëŠ” ëª¨ë“ˆ
================================================================

ì´ ëª¨ë“ˆì€ **LLMì´ ìƒì„±í•œ ì‘ë‹µ**ê³¼ **ì •ì±… ì…€ë ‰í„°ê°€ ê³ ë¥¸ ëŒ€ì²´í–‰ë™**ì„ í•©ì³,
ìš°ë¦¬ ì œí’ˆ ê·œì¹™(ì±„íŒ… 1í„´=ëŒ€ì²´í–‰ë™ 2~3ê°œ, ê¸€ììˆ˜/ë‹¨ë½/ì´ëª¨ì§€ ì œí•œ, ê¼¬ë¦¬ë©˜íŠ¸ ì´ëª¨ì§€ ë³´ì¥ ë“±)ì—
ë§ê²Œ **í›„ì²˜ë¦¬(post-processing)** í•©ë‹ˆë‹¤.

í•µì‹¬ íë¦„
---------
1) `prepare_alternatives()`  
   - LLM í›„ë³´ì™€ ì •ì±… í›„ë³´ë¥¼ **ë¨¸ì§€**í•˜ê³ ,  
   - ë¬¸ì²´ë¥¼ **~í•˜ê¸°** í˜•íƒœë¡œ ìµœëŒ€í•œ í†µì¼,  
   - **ê¸¸ì´ ì œí•œ(â‰¤90ì)**, **ì¤‘ë³µ ì œê±°(í† í° ìì¹´ë“œ ìœ ì‚¬ë„)**, **ê¸ˆì¹™ì–´ í•„í„°** ë“±ì„ ì ìš©í•˜ì—¬  
   - ê·œì¹™ì— ë§ëŠ” **2~3ê°œ** ëŒ€ì²´í–‰ë™ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.

2) `enforce_response()`  
   - summary/fox_tailì— ëŒ€í•´ **ë‹¨ë½(â‰¤2)**, **ì´ëª¨ì§€(â‰¤2)**, **ê¸¸ì´(ìš”ì•½ â‰¤300ì)** ì œí•œê³¼  
   - ê¼¬ë¦¬ë©˜íŠ¸ **ì—¬ìš° ì´ëª¨ì§€(ğŸ¦Š)** ë³´ì¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

3) `postprocess_bundle()`  
   - (ê¶Œì¥) í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” í¸ì˜ í•¨ìˆ˜ì…ë‹ˆë‹¤.  
   - `prepare_alternatives()` + `enforce_response()` ë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì¢… JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤.

ì˜ì¡´ì„±
------
- ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ(ì •ê·œì‹ë§Œ ì‚¬ìš©).
- í† í°í™”/ìœ ì‚¬ë„ëŠ” ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±(ê³µë°±/í•œê¸€/ì˜ë¬¸/ìˆ«ì í† í°)ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì£¼ì˜
----
- ë¬¸ì²´ í†µì¼(~í•˜ê¸°)ì€ **ë³´ìˆ˜ì **ìœ¼ë¡œ ì ìš©í•©ë‹ˆë‹¤. ì–´ìƒ‰í•œ í•œêµ­ì–´ ìƒì„±ì„ í”¼í•˜ê¸° ìœ„í•´
  íŠ¹ì • ì–´ë¯¸(í•˜ì„¸ìš”/í•©ë‹ˆë‹¤ ë“±)ë§Œ ì•ˆì „í•˜ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.
- ê¸ˆì¹™ì–´ í•„í„°ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ `*******` ë§ˆìŠ¤í‚¹ì…ë‹ˆë‹¤. í•„ìš” ì‹œ ë™ì˜ì–´ ì¹˜í™˜ì„ ë¶™ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

from __future__ import annotations
import re
from typing import List, Dict, Any

# --------------------------------------------------------------------
# 0) ì •ê·œì‹ ì¤€ë¹„
# --------------------------------------------------------------------
# ë‹¤ì¤‘ ê³µë°±ì„ 1ì¹¸ìœ¼ë¡œ
_WHITESPACE_RE = re.compile(r"\s+")
# ... ë˜ëŠ” â€¦ë¥¼ í†µì¼
_ELLIPSIS_RE = re.compile(r"\u2026|\.{3,}")
# ì´ëª¨ì§€ ëŒ€ëµ ë²”ìœ„(ì™„ë²½í•˜ì§€ ì•Šì§€ë§Œ ì¶©ë¶„)
_EMOJI_RE = re.compile(r"[\U0001F300-\U0001FAFF]")

# --------------------------------------------------------------------
# 1) ê³µí†µ í…ìŠ¤íŠ¸ ìœ í‹¸
# --------------------------------------------------------------------
def normalize_spaces(text: str) -> str:
    """ì—°ì†ëœ ê³µë°±/ê°œí–‰ì„ 1ì¹¸ ê³µë°±ìœ¼ë¡œ ì •ê·œí™”."""
    return _WHITESPACE_RE.sub(" ", (text or "").strip())

def clamp_len(text: str, max_chars: int) -> str:
    """ë¬¸ìì—´ ê¸¸ì´ë¥¼ max_chars ì´í•˜ë¡œ ìë¦…ë‹ˆë‹¤(ë§ì¤„ì„í‘œ â€¦ ì‚¬ìš©)."""
    text = text or ""
    if len(text) <= max_chars:
        return text
    return text[: max_chars - 1] + "â€¦"

def ensure_tail_emoji(text: str, emoji: str = "ğŸ¦Š") -> str:
    """ë¬¸ì¥ ëì— íŠ¹ì • ì´ëª¨ì§€(ê¸°ë³¸ ğŸ¦Š)ê°€ ì—†ìœ¼ë©´ ë¶™ì—¬ ì¤ë‹ˆë‹¤."""
    text = (text or "").strip()
    if not text.endswith(emoji):
        if text.endswith("."):
            text = text[:-1]
        text = (text + " " + emoji).strip()
    return text

def limit_paragraphs(text: str, max_paragraphs: int = 2) -> str:
    """ë‘ ì¤„ ì´ìƒ ë„ì–´ ì“´ ë‹¨ë½ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ ìµœëŒ€ max_paragraphs ë‹¨ë½ë§Œ ë‚¨ê¹ë‹ˆë‹¤."""
    paras = [p.strip() for p in re.split(r"\n{2,}", text or "") if p.strip()]
    return "\n\n".join(paras[:max_paragraphs]) if paras else (text or "").strip()

def limit_emojis(text: str, max_total: int = 2) -> str:
    """
    í…ìŠ¤íŠ¸ì˜ ì´ëª¨ì§€ ê°œìˆ˜ë¥¼ ìµœëŒ€ max_totalë¡œ ì œí•œí•©ë‹ˆë‹¤.
    ê°„ë‹¨í•˜ê²Œ ì´ˆê³¼ë¶„ì„ ì œê±°í•©ë‹ˆë‹¤(í‘œí˜„ ë³´ì¡´ë³´ë‹¤ ì •ì±… ì¤€ìˆ˜ë¥¼ ìš°ì„ ).
    """
    out = []
    cnt = 0
    for ch in text:
        if _EMOJI_RE.match(ch):
            cnt += 1
            if cnt > max_total:
                continue
        out.append(ch)
    return "".join(out)

# --------------------------------------------------------------------
# 2) ë¬¸ì²´ í†µì¼(~í•˜ê¸°) â€” ê³¼ë„í•œ ë³€í™˜ì„ í”¼í•˜ëŠ” ê°€ë²¼ìš´ íœ´ë¦¬ìŠ¤í‹±
# --------------------------------------------------------------------
def to_verb_noun_style(s: str) -> str:
    """
    ê°€ë²¼ìš´ ê·œì¹™ë§Œ ì ìš©í•´ '~í•˜ê¸°/í•´ë³´ê¸°' í˜•íƒœì— ê°€ê¹ê²Œ ë‹¤ë“¬ìŠµë‹ˆë‹¤.
    - ì´ë¯¸ 'í•˜ê¸°/í•´ë³´ê¸°/í•´ ë‘ê¸°/ì¼œ ë‘ê¸°/ì“°ê¸°'ë¡œ ëë‚˜ë©´ ê·¸ëŒ€ë¡œ ë‘ 
    - 'í•´ë³´ì„¸ìš”/í•˜ì„¸ìš”/í•©ë‹ˆë‹¤' ë“±ì˜ ì¢…ê²° ì–´ë¯¸ë¥¼ ì•ˆì „í•˜ê²Œ 'í•˜ê¸°/í•´ë³´ê¸°'ë¡œ ë³€í™˜
    - ë„ˆë¬´ ì–´ìƒ‰í•´ì§ˆ ìœ„í—˜ì´ ìˆìœ¼ë©´ ì›ë¬¸ ìœ ì§€
    """
    s = normalize_spaces(s)
    s = _ELLIPSIS_RE.sub("â€¦", s)

    # ì´ë¯¸ ëª…ì‚¬í˜•ì´ë©´ ê·¸ëŒ€ë¡œ
    if re.search(r"(í•˜ê¸°|í•´ë³´ê¸°|í•´ ë‘ê¸°|ì¼œ ë‘ê¸°|ì“°ê¸°)$", s):
        return s

    # ì•ˆì „í•œ ì¢…ê²° ì–´ë¯¸ ë³€í™˜
    replacements = [
        (r"í•´ë³´ì„¸ìš”$", "í•´ë³´ê¸°"),
        (r"í•´ìš”$", "í•˜ê¸°"),
        (r"í•˜ì„¸ìš”$", "í•˜ê¸°"),
        (r"í•©ë‹ˆë‹¤$", "í•˜ê¸°"),
        (r"í•©ë‹ˆë‹¤\.$", "í•˜ê¸°"),
        (r"í•©ì‹œë‹¤$", "í•˜ê¸°"),
        (r"í•˜ì„¸ìš”\.$", "í•˜ê¸°"),
        (r"í•˜ê¸°\.$", "í•˜ê¸°"),
    ]
    for pat, rep in replacements:
        if re.search(pat, s):
            return re.sub(pat, rep, s)

    # ì´ë¯¸ 'ê¸°'ë¡œ ëë‚˜ë©´ ìœ ì§€(ex. 'ì„¤ì •í•˜ê¸°', 'ê¸°ë¡í•˜ê¸°')
    if re.search(r"(ê¸°)$", s):
        return s

    # ì§§ì€ ì„ ì–¸ë¬¸(â€¦ë‹¤)ì¸ ê²½ìš°ë§Œ ë¶€ë“œëŸ½ê²Œ ë³€í™˜
    if len(s) <= 20 and s.endswith("ë‹¤"):
        return s[:-1] + "í•˜ê¸°"

    # ê·¸ ì™¸ëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ ì›ë¬¸ ìœ ì§€
    return s

# --------------------------------------------------------------------
# 3) ê¸ˆì¹™ì–´ í•„í„°
# --------------------------------------------------------------------
def filter_forbidden(text: str, forbidden_words: List[str]) -> str:
    """
    ê¸ˆì¹™ì–´ ëª©ë¡ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ë³„í‘œ(******)ë¡œ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤.
    - ê°„ë‹¨í•œ substring ì¹˜í™˜(ë‹¨ì–´ ê²½ê³„ íŒë‹¨ ì—†ìŒ) â€” ì •ì±… ì¤€ìˆ˜ ëª©ì .
    - í•„ìš”í•˜ë©´ ì‚¬ì „ ê¸°ë°˜ ë™ì˜ì–´ ì¹˜í™˜ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŒ.
    """
    out = text
    for w in forbidden_words or []:
        if not w.strip():
            continue
        out = out.replace(w, "*" * len(w))
    return out

# --------------------------------------------------------------------
# 4) ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°(í† í° ìì¹´ë“œ)
# --------------------------------------------------------------------
def _tokens(x: str) -> set:
    """í•œê¸€/ì˜ë¬¸/ìˆ«ì í† í°ë§Œ ë½‘ì•„ setìœ¼ë¡œ ë³€í™˜."""
    return set(re.findall(r"[ê°€-í£A-Za-z0-9]+", x.lower()))

def jaccard_sim(a: str, b: str) -> float:
    """ìì¹´ë“œ ìœ ì‚¬ë„: êµì§‘í•©/í•©ì§‘í•©."""
    A, B = _tokens(a), _tokens(b)
    if not A or not B:
        return 0.0
    return len(A & B) / len(A | B)

def dedup_similar(items: List[str], threshold: float = 0.65) -> List[str]:
    """
    ì˜ë¯¸ê°€ ê±°ì˜ ê°™ì€ í•­ëª©ì„ ì œê±°í•©ë‹ˆë‹¤.
    - threshold(ê¸°ë³¸ 0.65) ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨.
    - ìˆœì„œëŒ€ë¡œ ë³´ë©´ì„œ, ì´ë¯¸ ì±„íƒëœ í•­ëª©ê³¼ì˜ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    """
    out: List[str] = []
    for s in items:
        s_norm = normalize_spaces(s)
        if not s_norm:
            continue
        duplicate = any(jaccard_sim(s_norm, t) >= threshold for t in out)
        if not duplicate:
            out.append(s_norm)
    return out

# --------------------------------------------------------------------
# 5) ëŒ€ì²´í–‰ë™ ë¨¸ì§€ & ì •ë¦¬
# --------------------------------------------------------------------
def prepare_alternatives(
    llm_items: List[str] | None,
    policy_items: List[str] | None,
    k_min: int = 2,
    k_max: int = 3,
    item_max_chars: int = 90,
    forbidden_words: List[str] | None = None,
) -> List[str]:
    """
    LLM í›„ë³´ì™€ ì •ì±… í›„ë³´ë¥¼ í•©ì³ 'ê¹¨ë—í•œ' 2~3ê°œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    1) í•©ì¹˜ê¸° â†’ 2) ë¬¸ì²´(~í•˜ê¸°) â†’ 3) ê¸¸ì´ ì œí•œ â†’ 4) ì •í™• ì¤‘ë³µ ì œê±° â†’ 5) ì˜ë¯¸ ì¤‘ë³µ ì œê±° â†’ 6) ê¸ˆì¹™ì–´ â†’ 7) ê°œìˆ˜ ë³´ì¥
    """
    merged = [*(llm_items or []), *(policy_items or [])]

    # 2) ë¬¸ì²´/ê¸¸ì´/ì •í™• ì¤‘ë³µ ì œê±°
    cleaned = []
    seen = set()
    for s in merged:
        if not s:
            continue
        s = to_verb_noun_style(s)
        s = clamp_len(s, item_max_chars)
        s = normalize_spaces(s)
        if s in seen:
            continue
        seen.add(s)
        cleaned.append(s)

    # 3) ì˜ë¯¸ ì¤‘ë³µ ì œê±°(ìì¹´ë“œ ìœ ì‚¬ë„)
    cleaned = dedup_similar(cleaned, threshold=0.65)

    # 4) ê¸ˆì¹™ì–´ í•„í„°(ì˜µì…˜)
    if forbidden_words:
        cleaned = [filter_forbidden(s, forbidden_words) for s in cleaned]

    # 5) 2~3ê°œ ë³´ì¥(ë¶€ì¡±í•˜ë©´ ì•ˆì „í•œ ê¸°ë³¸ í•­ëª©ìœ¼ë¡œ íŒ¨ë”©)
    SAFE_PAD = ["ë¬¼ í•œ ì»µ ë§ˆì‹œê¸°", "5ë¶„ íƒ€ì´ë¨¸ ì„¤ì •í•˜ê¸°", "ê¹Ší˜¸í¡ 3íšŒí•˜ê¸°"]
    while len(cleaned) < max(2, k_min) and SAFE_PAD:
        s = SAFE_PAD.pop(0)
        if s not in cleaned:
            cleaned.append(s)

    # 6) ìµœì¢… ê°œìˆ˜ ì»·
    return cleaned[: max(2, min(3, k_max))]

# --------------------------------------------------------------------
# 6) summary/fox_tail ìµœì¢… ì •ë¦¬
# --------------------------------------------------------------------
def enforce_response(
    summary: str,
    alternatives: List[str],
    fox_tail: str,
    max_paragraphs: int = 2,
    max_emojis: int = 2,
    summary_max_chars: int = 300,
    forbidden_words: List[str] | None = None,
) -> Dict[str, Any]:
    """
    - summary: ê³µë°±/ë‹¨ë½/ê¸¸ì´/ì´ëª¨ì§€/ê¸ˆì¹™ì–´ ì²˜ë¦¬
    - fox_tail: ê³µë°±/ì´ëª¨ì§€ ì œí•œ + ê¼¬ë¦¬ ì´ëª¨ì§€(ğŸ¦Š) ë³´ì¥ + ê¸ˆì¹™ì–´ ì²˜ë¦¬
    - alternatives: ì´ë¯¸ `prepare_alternatives()`ì—ì„œ ì •ë¦¬ëœ ìƒíƒœë¥¼ ì‚¬ìš©
    """
    # summary ì •ë¦¬
    summary = normalize_spaces(summary)
    summary = limit_paragraphs(summary, max_paragraphs=max_paragraphs)
    summary = clamp_len(summary, summary_max_chars)
    summary = limit_emojis(summary, max_total=max_emojis)
    if forbidden_words:
        summary = filter_forbidden(summary, forbidden_words)

    # fox_tail ì •ë¦¬
    fox_tail = normalize_spaces(fox_tail or "ì ê¹ ë©ˆì¶”ë©´ ë” ì˜ ë³´ì—¬ ğŸ¦Š")
    fox_tail = limit_emojis(fox_tail, max_total=max_emojis)
    fox_tail = ensure_tail_emoji(fox_tail, emoji="ğŸ¦Š")
    if forbidden_words:
        fox_tail = filter_forbidden(fox_tail, forbidden_words)

    return {"summary": summary, "alternatives": alternatives, "fox_tail": fox_tail}

# --------------------------------------------------------------------
# 7) ë²ˆë“¤ í¸ì˜ í•¨ìˆ˜(ê¶Œì¥)
# --------------------------------------------------------------------
def postprocess_bundle(
    llm_alternatives: List[str] | None,
    policy_alternatives: List[str] | None,
    summary: str,
    fox_tail: str,
    cfg: Dict[str, Any] | None = None,
    forbidden_words: List[str] | None = None,
) -> Dict[str, Any]:
    """
    prompts.yamlì—ì„œ ê°€ì ¸ì˜¨ ì œì•½ì„ ì ìš©í•´ í•œ ë²ˆì— í›„ì²˜ë¦¬í•©ë‹ˆë‹¤.
    - cfg.defaults.list_constraints: {alternatives_min, alternatives_max, alternatives_item_max_chars}
    - cfg.defaults.length_constraints: {max_paragraphs}
    - cfg.defaults.emoji_policy: {max_total}
    - cfg.output_contract.summary_max_chars
    """
    cfg = cfg or {}
    defaults = cfg.get("defaults", {})
    lst = defaults.get("list_constraints", {})
    lc = defaults.get("length_constraints", {})
    emoji = defaults.get("emoji_policy", {})

    item_max = lst.get("alternatives_item_max_chars", 90)
    k_min = lst.get("alternatives_min", 2)
    k_max = lst.get("alternatives_max", 3)
    max_par = lc.get("max_paragraphs", 2)
    max_emj = emoji.get("max_total", 2)
    sum_max = cfg.get("output_contract", {}).get("summary_max_chars", 300) or 300

    final_alts = prepare_alternatives(
        llm_items=llm_alternatives,
        policy_items=policy_alternatives,
        k_min=k_min,
        k_max=k_max,
        item_max_chars=item_max,
        forbidden_words=forbidden_words,
    )
    return enforce_response(
        summary=summary,
        alternatives=final_alts,
        fox_tail=fox_tail,
        max_paragraphs=max_par,
        max_emojis=max_emj,
        summary_max_chars=sum_max,
        forbidden_words=forbidden_words,
    )
