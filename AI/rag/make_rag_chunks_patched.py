# make_rag_chunks.py
import os, json, hashlib, uuid, re
from pathlib import Path
from typing import List, Dict, Any

# ìµœì‹  import (langchain-community)
from langchain_community.document_loaders import (
    DirectoryLoader, PyPDFLoader, TextLoader, UnstructuredMarkdownLoader, NotebookLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter

# (ì„ íƒ) í† í° ê¸¸ì´ ì¸¡ì •
try:
    import tiktoken
    enc = tiktoken.get_encoding("cl100k_base")
except Exception:
    enc = None

# ====== ì„¤ì • ======
INPUT_DIR = "./papers"            # ì…ë ¥ í´ë”
OUTPUT_JSONL = "./chunks.jsonl"   # JSONL ê²°ê³¼
OUTPUT_TXT_DIR = "./chunks_txt"   # ì²­í¬ TXT ì €ì¥ í´ë”
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 150
EXTS = ("pdf",)                   # í•„ìš” ì‹œ ("pdf","txt","md","ipynb")
DOC_ID_PREFIX = "paper"
MIN_CHUNK_CHARS = 100             # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œê±° ê¸°ì¤€
# ===================

LOADER_DICT = {
    "pdf": PyPDFLoader,
    "txt": TextLoader,
    "md": UnstructuredMarkdownLoader,
    "ipynb": NotebookLoader,
}



def decode_slashC_encoding(s: str) -> str:
    import re
    return re.sub(r"/C(\d{1,3})", lambda m: chr(int(m.group(1))), s)
def clean_text(text: str) -> str:
    # 1) Fix '/C###' decimal-escaped chars
    text = decode_slashC_encoding(text)
    # 2) Basic whitespace normalization
    return " ".join(text.split())

def load_documents(folder: str, exts=EXTS):
    docs = []
    for ext in exts:
        loader_cls = LOADER_DICT[ext]
        loader = DirectoryLoader(folder, glob=f"*.{ext}", loader_cls=loader_cls, show_progress=True)
        loaded = loader.load()
        for d in loaded:
            d.page_content = clean_text(d.page_content)
            src = d.metadata.get("source", "")
            d.metadata["filename"] = Path(src).name if src else "unknown"
        docs.extend(loaded)
    return docs

def split_documents(docs):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", " ", ""],
    )
    return splitter.split_documents(docs)

def md5_of_text(text: str) -> str:
    import hashlib
    return hashlib.md5(text.encode("utf-8")).hexdigest()

def token_len(text: str) -> int:
    return len(enc.encode(text)) if enc else 0

def build_items(split_docs) -> List[Dict[str, Any]]:
    out = []
    name2id = {}
    for i, d in enumerate(split_docs):
        filename = d.metadata.get("filename", "unknown")
        page = d.metadata.get("page", None)
        if filename not in name2id:
            name2id[filename] = f"{DOC_ID_PREFIX}-{uuid.uuid4().hex[:8]}"
        doc_id = name2id[filename]

        content = (d.page_content or "").strip()
        if not content or len(content) < MIN_CHUNK_CHARS:
            continue

        h = md5_of_text(f"{filename}|{page}|{content[:600]}")
        out.append({
            "doc_id": doc_id,
            "filename": filename,
            "page": page,
            "chunk_id": i,
            "content": content,
            "num_tokens": token_len(content),
            "source": d.metadata.get("source", filename),
            "hash": h,
        })
    return out

def dedupe_by_hash(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen, uniq = set(), []
    for it in items:
        if it["hash"] in seen:
            continue
        seen.add(it["hash"])
        uniq.append(it)
    return uniq

def save_jsonl(items: List[Dict[str, Any]], path: str):
    Path(path).parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(it, ensure_ascii=False) + "\n")

def safe_slug(name: str, maxlen: int = 40) -> str:
    # íŒŒì¼ëª… ì•ˆì „ ë¬¸ìì—´ (ì˜ë¬¸/ìˆ«ì/_/-ë§Œ) + ê¸¸ì´ ì œí•œ
    base = re.sub(r"[^A-Za-z0-9_\-]+", "_", name)
    return base[:maxlen].strip("_") or "chunk"

def save_chunks_as_txt(items: List[Dict[str, Any]], outdir: str):
    odir = Path(outdir)
    odir.mkdir(parents=True, exist_ok=True)
    counters = {}
    for it in items:
        # íŒŒì¼ëª…: <docslug>_p<page>_c<chunkid>.txt (ì¤‘ë³µ ë°©ì§€ ì¹´ìš´í„° í¬í•¨)
        docslug = safe_slug(Path(it["filename"]).stem)
        page = it.get("page")
        page_part = f"p{page}" if page is not None else "pNA"
        base = f"{docslug}_{page_part}_c{it['chunk_id']}"
        # í˜¹ì‹œ ë™ì¼ ì´ë¦„ ì¶©ëŒ ì‹œ ë’¤ì— -n
        counters.setdefault(base, 0)
        counters[base] += 1
        suffix = f"-{counters[base]}" if counters[base] > 1 else ""
        fname = f"{base}{suffix}.txt"

        # (ì„ íƒ) ë©”íƒ€ë°ì´í„° í—¤ë”ë¥¼ íŒŒì¼ ìƒë‹¨ì— í¬í•¨í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
        # header = (
        #     f"# doc_id: {it['doc_id']}\n"
        #     f"# filename: {it['filename']}\n"
        #     f"# page: {it['page']}\n"
        #     f"# chunk_id: {it['chunk_id']}\n"
        #     f"# tokens: {it['num_tokens']}\n"
        #     f"# source: {it['source']}\n"
        #     f"# hash: {it['hash']}\n\n"
        # )
        # text = header + it["content"]

        text = it["content"]  # í—¤ë” ì—†ì´ ë³¸ë¬¸ë§Œ ì €ì¥
        (odir / fname).write_text(text, encoding="utf-8")

def main():
    if not Path(INPUT_DIR).exists():
        raise FileNotFoundError(f"ì…ë ¥ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {INPUT_DIR}")

    print(f"[1/4] ë¬¸ì„œ ë¡œë”© ì¤‘...")
    docs = load_documents(INPUT_DIR, EXTS)
    print(f"  - ë¡œë”©ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")

    print(f"[2/4] ì²­í¬ ë¶„í•  ì¤‘ (size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})...")
    split_docs = split_documents(docs)
    print(f"  - ë¶„í•  í›„ ì²­í¬ ìˆ˜: {len(split_docs)}")

    print(f"[3/4] ë©”íƒ€ë°ì´í„°/í•´ì‹œ ë¶€ì°© & ì¤‘ë³µ ì œê±°...")
    items = dedupe_by_hash(build_items(split_docs))
    print(f"  - ìœ ë‹ˆí¬ ì²­í¬ ìˆ˜: {len(items)}")
    if not items:
        print("  - ì €ì¥í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤(ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹ˆ ì²­í¬). ì„¤ì •ê°’ì„ ì¡°ì •í•´ ë³´ì„¸ìš”.")
        return

    print(f"[4/4] ì €ì¥: JSONL -> {OUTPUT_JSONL}, TXT -> {OUTPUT_TXT_DIR}/")
    save_jsonl(items, OUTPUT_JSONL)
    save_chunks_as_txt(items, OUTPUT_TXT_DIR)
    print("ì™„ë£Œ! ğŸ‰")

if __name__ == "__main__":
    main()
