
"""
evaluator.py ‚Äî ÏãúÎÇòÎ¶¨Ïò§ ÏûêÎèô ÌèâÍ∞ÄÍ∏∞ (ÎùºÏù¥Ìä∏ Î≤ÑÏ†Ñ)

- ÏûÖÎ†•: scenarios.jsonl (Í∞Å ÏºÄÏù¥Ïä§Ïùò context/expected)
- Î™®Îç∏ Ïó∞Í≤∞: call_model(ctx) Ìï®ÏàòÏóê Ïã§Ï†ú ÌååÏù¥ÌîÑÎùºÏù∏ Ìò∏Ï∂úÏùÑ Íµ¨ÌòÑÌïòÍ±∞ÎÇò, mock_modelÏùÑ ÏÇ¨Ïö©
- ÌèâÍ∞Ä: Ïä§ÌÇ§Îßà/Ï†ïÏ±Ö Ï§ÄÏàò(Í∞úÏàò/Îã®ÎùΩ/Ïù¥Î™®ÏßÄ/ü¶ä) + ÎÇ¥Ïö© Ï†ÅÌï©ÏÑ±(Í∞ÑÎã® Ìú¥Î¶¨Ïä§Ìã±)
- Ï∂úÎ†•: JSONL Í≤∞Í≥º + HTML Î¶¨Ìè¨Ìä∏(ÌÖåÏù¥Î∏î)

Ïã§Ï†ú ÏÑúÎπÑÏä§ Ïó∞Îèô Ïãú:
- call_model(ctx) ‚Üí inference.pipeline.coach(request) Î°ú ÍµêÏ≤¥
- prompts.yaml Í∏àÏπôÏñ¥/Ï†úÏïΩÏùÄ config Î°úÎî© Îí§ ÌååÎùºÎØ∏ÌÑ∞Î°ú Ï£ºÏûÖ
"""

from __future__ import annotations
import os, json, re, datetime, html
from typing import Dict, Any, List, Tuple

HERE = os.path.dirname(__file__)
REPORT_DIR = os.path.join(HERE, "reports")

# ----------------------------
# 0) Í∞ÑÎã®Ìïú Î™®Ìòï: mock_model
# ----------------------------
def mock_model(ctx: Dict[str, Any]) -> Dict[str, Any]:
    """
    Îç∞Î™®Ïö© Í≥†Ï†ï Í∑úÏπô Ï∂úÎ†•:
    - summary: 1~2Î¨∏Ïû•, Í≥µÍ∞ê Ìïú ÎßàÎîî Ìè¨Ìï®
    - alternatives: BAS/ÎèÑÎ©îÏù∏ ÌûåÌä∏Ïóê ÎßûÎäî 2~3Í∞ú Í≥†Ï†ï ÏÑ∏Ìä∏
    - fox_tail: ü¶ä Î≥¥Ïû•
    """
    mood = ctx.get("mood", "")
    top = ctx.get("spending_top_category", "")
    # ÏïÑÏ£º Îã®ÏàúÌïú Îß§Ìïë
    if "Î∞∞Îã¨" in top or "ÏïºÏãù" in top:
        alts = ["Î¨º Ìïú Ïªµ ÎßàÏãúÍ∏∞", "Ï¶âÏÑùÏãùÏúºÎ°ú 10Î∂Ñ ÎÇ¥ Ìï¥Í≤∞ÌïòÍ∏∞"]
    elif "ÏáºÌïë" in top:
        alts = ["Ïû•Î∞îÍµ¨ÎãàÏóê Îã¥Í≥† 24ÏãúÍ∞Ñ Îí§ Îã§Ïãú Î≥¥Í∏∞", "ÎπÑÏä∑Ìïú Ï†úÌíà 2Í∞úÎßå ÎπÑÍµêÌëú ÏûëÏÑ±ÌïòÍ∏∞"]
    elif "Ïπ¥Ìéò" in top:
        alts = ["Ïßë/ÌöåÏÇ¨ Ìã∞Î∞±ÏúºÎ°ú ÎåÄÏ≤¥ÌïòÍ∏∞", "ÏÇ∞Ï±Ö 5Î∂Ñ + ÌÖÄÎ∏îÎü¨ Î¨º Ï±ÑÏö∞Í∏∞"]
    elif "Íµ¨ÎèÖ" in top:
        alts = ["ÏßÄÎÇú 30Ïùº ÏÇ¨Ïö© ÏãúÍ∞Ñ ÌôïÏù∏ÌïòÍ∏∞", "Ï≤¥ÌóòÌåêÏúºÎ°ú Î®ºÏ†Ä ÏÇ¨Ïö©Ìï¥ Î≥¥Í∏∞"]
    else:
        alts = ["5Î∂Ñ ÌÉÄÏù¥Î®∏ ÏÑ§Ï†ïÌïòÍ∏∞", "ÍπäÌò∏Ìù° 3ÌöåÌïòÍ∏∞"]

    summary = f"ÏßÄÍ∏à ÏÉÅÌô©Ïù¥ ÍΩ§ Î∂ÄÎã¥Ïä§Îü¨Ïõ†Í≤†ÎÑ§. {ctx.get('diary_summary','')[:40]}".strip()
    fox = "Ïû†Íπê Î©àÏ∂îÎ©¥ Îçî Ïûò Î≥¥Ïó¨ ü¶ä"
    return {"summary": summary, "alternatives": alts[:3], "fox_tail": fox}

def call_model(ctx: Dict[str, Any]) -> Dict[str, Any]:
    """
    Ïã§Ï†ú Ïó∞Í≤∞ ÏßÄÏ†ê.
    Ïòà: 
      from AI.inference.pipeline import coach
      return coach({...})
    ÌòÑÏû¨Îäî Îç∞Î™®Î•º ÏúÑÌï¥ mock_model ÏÇ¨Ïö©.
    """
    return mock_model(ctx)

# ----------------------------
# 1) ÌèâÍ∞Ä Ìï®ÏàòÎì§
# ----------------------------
EMOJI_RE = re.compile(r"[\U0001F300-\U0001FAFF]")

def count_paragraphs(text: str) -> int:
    paras = [p.strip() for p in re.split(r"\n{2,}", text or "") if p.strip()]
    return max(1, len(paras)) if text else 0

def count_emojis(text: str) -> int:
    return len(EMOJI_RE.findall(text or ""))

def in_range(n: int, lo: int, hi: int) -> bool:
    return lo <= n <= hi

def contains_any(text: str, keywords: List[str]) -> bool:
    t = text or ""
    return any(k in t for k in (keywords or []))

def evaluate_case(ctx: Dict[str, Any], expected: Dict[str, Any], resp: Dict[str, Any], forbidden: List[str] | None = None) -> Dict[str, Any]:
    # Ïä§ÌÇ§Îßà/Ï†ïÏ±Ö Ï§ÄÏàò
    alts = resp.get("alternatives", []) or []
    summary = resp.get("summary", "") or ""
    fox = resp.get("fox_tail", "") or ""
    a_ok = in_range(len(alts), expected["alts_count_range"][0], expected["alts_count_range"][1])
    p_ok = count_paragraphs(summary) <= expected["paragraphs_max"]
    e_ok = (count_emojis(summary) + count_emojis(fox)) <= 2
    f_ok = fox.endswith("ü¶ä")
    forb_ok = True
    if forbidden:
        forb_ok = not any(w in (summary + " " + " ".join(alts) + " " + fox) for w in forbidden)

    schema_score = sum([a_ok, p_ok, e_ok, f_ok, forb_ok]) / 5.0

    # ÎÇ¥Ïö© Ï†ÅÌï©ÏÑ± (ÏïÑÏ£º ÎùºÏù¥Ìä∏Ìïú Ìú¥Î¶¨Ïä§Ìã±)
    tone = expected.get("tone_contains", [])
    tone_ok = contains_any(summary, tone) or contains_any(" ".join(alts), tone)

    domain_hint = expected.get("domain_hint", [])
    dom_ok = contains_any(" ".join(alts), ["Î∞∞Îã¨","ÏïºÏãù","Ïû•Î∞îÍµ¨Îãà","Ìã∞Î∞±","Ï≤¥ÌóòÌåê","Ïø†Ìè∞","ÎπÑÍµêÌëú","ÏÇ∞Ï±Ö","ÌÉÄÏù¥Î®∏"])
    # ÏúÑ dom_okÎäî Í∞ÑÎã® ÌÇ§ÏõåÎìúÎ°ú ÎåÄÏ≤¥ (Ïã§Ï†úÎäî ÎèÑÎ©îÏù∏‚ÜíÌñâÎèô ÏÇ¨Ï†ÑÏúºÎ°ú ÌèâÍ∞Ä Í∂åÏû•)

    content_score = sum([tone_ok, dom_ok]) / 2.0

    # Ï¥ùÏ†ê(Í∞ÄÏ§ëÏπò): Ïä§ÌÇ§Îßà 0.6 + ÎÇ¥Ïö© 0.4
    total = round(schema_score * 0.6 + content_score * 0.4, 3)

    return {
        "schema": {
            "alternatives_count_ok": a_ok,
            "paragraphs_ok": p_ok,
            "emoji_budget_ok": e_ok,
            "fox_tail_ok": f_ok,
            "forbidden_ok": forb_ok,
            "score": round(schema_score, 3)
        },
        "content": {
            "tone_ok": tone_ok,
            "domain_ok": dom_ok,
            "score": round(content_score, 3)
        },
        "total": total,
        "response": resp
    }

# ----------------------------
# 2) Ïã§Ìñâ ÏßÑÏûÖÏ†ê
# ----------------------------
def run_eval(scenarios_path: str | None = None, report_dir: str | None = None, forbidden: List[str] | None = None) -> str:
    scenarios_path = scenarios_path or os.path.join(HERE, "scenarios.jsonl")
    report_dir = report_dir or REPORT_DIR
    os.makedirs(report_dir, exist_ok=True)

    results = []
    with open(scenarios_path, "r", encoding="utf-8") as f:
        for line in f:
            sc = json.loads(line)
            ctx = sc["context"]
            expected = sc["expected"]
            resp = call_model(ctx)
            res = evaluate_case(ctx, expected, resp, forbidden=forbidden)
            results.append({"id": sc["id"], "title": sc["title"], "ctx": ctx, "expected": expected, "eval": res})

    # Í≤∞Í≥º Ï†ÄÏû•(JSONL)
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    out_jsonl = os.path.join(report_dir, f"results_{ts}.jsonl")
    with open(out_jsonl, "w", encoding="utf-8") as f:
        for r in results:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

    # HTML Î¶¨Ìè¨Ìä∏
    out_html = os.path.join(report_dir, f"report_{ts}.html")
    write_html_report(results, out_html)

    return out_html

def write_html_report(results: List[Dict[str, Any]], path: str) -> None:
    rows = []
    for r in results:
        e = r["eval"]
        resp = e["response"]
        schema = e["schema"]; content = e["content"]
        rows.append(f"""
<tr>
  <td>{html.escape(r['id'])}</td>
  <td>{html.escape(r['title'])}</td>
  <td>{e['total']}</td>
  <td>{schema['score']}</td>
  <td>{content['score']}</td>
  <td><pre style='white-space:pre-wrap'>{html.escape(resp.get('summary',''))}</pre></td>
  <td><ul>{"".join(f"<li>{html.escape(a)}</li>" for a in (resp.get('alternatives') or []))}</ul></td>
  <td>{html.escape(resp.get('fox_tail',''))}</td>
</tr>
""")
    html_doc = f"""<!doctype html>
<html lang="ko"><head>
<meta charset="utf-8"><title>Coach Eval Report</title>
<style>
body{{font-family:system-ui, -apple-system, Segoe UI, Roboto, Noto Sans KR, sans-serif}}
table{{border-collapse:collapse;width:100%}}
th,td{{border:1px solid #ddd;padding:8px;vertical-align:top}}
th{{background:#f7f7f7}}
pre{{margin:0}}
</style>
</head><body>
<h1>Coach Eval Report</h1>
<p>Ï¥ù ÏºÄÏù¥Ïä§: {len(results)}</p>
<table>
<thead>
<tr>
  <th>ID</th><th>Title</th><th>Total</th><th>Schema</th><th>Content</th>
  <th>Summary</th><th>Alternatives</th><th>Fox tail</th>
</tr>
</thead>
<tbody>
{''.join(rows)}
</tbody>
</table>
</body></html>"""
    with open(path, "w", encoding="utf-8") as f:
        f.write(html_doc)

if __name__ == "__main__":
    out = run_eval(forbidden=["ÎπöÏùÑ ÎÇ¥ÏÑú ÏÇ¨", "Í∞ïÏ†ú Íµ¨Îß§"])
    print("HTML report:", out)
